{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d642ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "‚úÖ Full dataset with 11552 atoms loaded.\n",
      "‚úÖ Backbone DataFrame created with 760 atoms.\n",
      "Creating 512 location tokens...\n",
      "‚úÖ Tokenization complete.\n",
      "\n",
      "--- Final Prepared Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain_id</th>\n",
       "      <th>residue_name</th>\n",
       "      <th>residue_seq_id</th>\n",
       "      <th>atom_name</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>z_coord</th>\n",
       "      <th>bfactor</th>\n",
       "      <th>location_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1l2y</td>\n",
       "      <td>A</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>-8.608</td>\n",
       "      <td>3.135</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1l2y</td>\n",
       "      <td>A</td>\n",
       "      <td>LEU</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>-4.923</td>\n",
       "      <td>4.002</td>\n",
       "      <td>-2.452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1l2y</td>\n",
       "      <td>A</td>\n",
       "      <td>TYR</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>-3.690</td>\n",
       "      <td>2.738</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1l2y</td>\n",
       "      <td>A</td>\n",
       "      <td>ILE</td>\n",
       "      <td>4</td>\n",
       "      <td>CA</td>\n",
       "      <td>-5.857</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1l2y</td>\n",
       "      <td>A</td>\n",
       "      <td>GLN</td>\n",
       "      <td>5</td>\n",
       "      <td>CA</td>\n",
       "      <td>-4.122</td>\n",
       "      <td>-1.167</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pdb_id chain_id residue_name  residue_seq_id atom_name  x_coord  y_coord  \\\n",
       "1    1l2y        A          ASN               1        CA   -8.608    3.135   \n",
       "17   1l2y        A          LEU               2        CA   -4.923    4.002   \n",
       "36   1l2y        A          TYR               3        CA   -3.690    2.738   \n",
       "57   1l2y        A          ILE               4        CA   -5.857   -0.449   \n",
       "76   1l2y        A          GLN               5        CA   -4.122   -1.167   \n",
       "\n",
       "    z_coord  bfactor  location_token  \n",
       "1    -1.618      0.0             161  \n",
       "17   -2.452      0.0             329  \n",
       "36    0.981      0.0             266  \n",
       "57    0.613      0.0             227  \n",
       "76   -2.743      0.0             345  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the Processed Data ---\n",
    "print(\"Loading processed data...\")\n",
    "data_path = Path('../data/processed/protein_structures.parquet')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"‚ùå ERROR: File not found at {data_path}\")\n",
    "    print(\"Please run 'python run.py' in your terminal first.\")\n",
    "else:\n",
    "    df = pd.read_parquet(data_path)\n",
    "    print(f\"‚úÖ Full dataset with {len(df)} atoms loaded.\")\n",
    "\n",
    "    # --- 2. Isolate the Protein Backbone ('CA' atoms) ---\n",
    "    backbone_df = df[df['atom_name'] == 'CA'].copy()\n",
    "    print(f\"‚úÖ Backbone DataFrame created with {len(backbone_df)} atoms.\")\n",
    "\n",
    "    # --- 3. Create Location Tokens using K-Means ---\n",
    "    # The number of clusters must be less than the number of samples (atoms)\n",
    "    if len(backbone_df) > 0:\n",
    "        num_location_tokens = 512 # You can adjust this value\n",
    "        if len(backbone_df) < num_location_tokens:\n",
    "            num_location_tokens = len(backbone_df) # Prevent error if samples < clusters\n",
    "\n",
    "        print(f\"Creating {num_location_tokens} location tokens...\")\n",
    "        \n",
    "        coords = backbone_df[['x_coord', 'y_coord', 'z_coord']].values\n",
    "        kmeans = KMeans(n_clusters=num_location_tokens, random_state=42, n_init='auto')\n",
    "        kmeans.fit(coords)\n",
    "        \n",
    "        # Add the tokens to our DataFrame\n",
    "        backbone_df['location_token'] = kmeans.predict(coords)\n",
    "        \n",
    "        print(\"‚úÖ Tokenization complete.\")\n",
    "        print(\"\\n--- Final Prepared Data ---\")\n",
    "        display(backbone_df.head())\n",
    "    else:\n",
    "        print(\"‚ùå Backbone DataFrame is empty. Cannot proceed with tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89eb4001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 individual protein token sequences.\n",
      "\n",
      "‚úÖ Training data created successfully.\n",
      "Shape of our input data (X): (632, 128)\n",
      "Shape of our target data (y): (632, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SEQUENCE_LENGTH = 128 \n",
    "sequences = backbone_df.groupby('pdb_id')['location_token'].apply(list).tolist()\n",
    "\n",
    "print(f\"Created {len(sequences)} individual protein token sequences.\")\n",
    "X = [] \n",
    "y = [] \n",
    "\n",
    "for seq in sequences:\n",
    "    \n",
    "    if len(seq) > SEQUENCE_LENGTH:\n",
    "        \n",
    "        for i in range(len(seq) - SEQUENCE_LENGTH):\n",
    "            X.append(seq[i:i + SEQUENCE_LENGTH])\n",
    "            y.append(seq[i + 1:i + SEQUENCE_LENGTH + 1])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"\\n‚úÖ Training data created successfully.\")\n",
    "print(f\"Shape of our input data (X): {X.shape}\")\n",
    "print(f\"Shape of our target data (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\aabharan\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aabharan\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\aabharan\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aabharan\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aabharan\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "  Downloading torchaudio-2.6.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "  Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "  Downloading torchaudio-2.5.0-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "  Downloading torchaudio-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "  Downloading torchaudio-2.4.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.3.1-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.3.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.2.2-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aabharan\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchaudio-2.2.2-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 33.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.7.1\n",
      "    Uninstalling torchaudio-2.7.1:\n",
      "      Successfully uninstalled torchaudio-2.7.1\n",
      "Successfully installed torchaudio-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ ProteusTransformer model created successfully!\n",
      "\n",
      "--- Model Architecture ---\n",
      "ProteusTransformer(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embedding): Embedding(512, 256)\n",
      "  (linear): Linear(in_features=256, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ProteusTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 512    \n",
    "D_MODEL = 256       \n",
    "N_HEAD = 8          \n",
    "D_HID = 512         \n",
    "N_LAYERS = 4        \n",
    "DROPOUT = 0.1       \n",
    "\n",
    "\n",
    "model = ProteusTransformer(VOCAB_SIZE, D_MODEL, N_HEAD, D_HID, N_LAYERS, DROPOUT)\n",
    "\n",
    "print(\"‚úÖ ProteusTransformer model created successfully!\")\n",
    "print(\"\\n--- Model Architecture ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7157c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "Epoch 1/10 | Average Loss: 3.5308\n",
      "Epoch 2/10 | Average Loss: 0.6340\n",
      "Epoch 3/10 | Average Loss: 0.2273\n",
      "Epoch 4/10 | Average Loss: 0.1661\n",
      "Epoch 5/10 | Average Loss: 0.1475\n",
      "Epoch 6/10 | Average Loss: 0.1383\n",
      "Epoch 7/10 | Average Loss: 0.1321\n",
      "Epoch 8/10 | Average Loss: 0.1294\n",
      "Epoch 9/10 | Average Loss: 0.1252\n",
      "Epoch 10/10 | Average Loss: 0.1242\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "X_tensor = torch.from_numpy(X).long()\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "EPOCHS = 10 #\n",
    "\n",
    "print(\"--- Starting Model Training ---\")\n",
    "\n",
    "model.train() \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), targets.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b038dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Generating new protein sequence...\n",
      "‚úÖ New protein token sequence generated successfully!\n",
      "\n",
      "--- Generated Token Sequence (first 15 tokens) ---\n",
      "[161, 186, 88, 295, 397, 71, 134, 342, 106, 139, 382, 99, 109, 428, 479]\n",
      "\n",
      "‚≠êÔ∏è Your new protein has been saved to: ../results/generated_proteins/novel_protein_1.xyz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model.eval() \n",
    "\n",
    "input_sequence = torch.from_numpy(X[0, :1]).long().unsqueeze(0)\n",
    "\n",
    "\n",
    "generated_tokens = input_sequence[0].tolist()\n",
    "\n",
    "\n",
    "GENERATION_LENGTH = 200\n",
    "TEMPERATURE = 0.8 \n",
    "\n",
    "print(\"üß¨ Generating new protein sequence...\")\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for _ in range(GENERATION_LENGTH):\n",
    "        \n",
    "        output = model(input_sequence)\n",
    "        \n",
    "        \n",
    "        last_token_logits = output[0, -1, :]\n",
    "        \n",
    "       \n",
    "        scaled_logits = last_token_logits / TEMPERATURE\n",
    "        probabilities = torch.nn.functional.softmax(scaled_logits, dim=0)\n",
    "        \n",
    "        \n",
    "        next_token = torch.multinomial(probabilities, 1)\n",
    "        \n",
    "        \n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        \n",
    "        input_sequence = torch.tensor([generated_tokens]).long()\n",
    "\n",
    "print(\"‚úÖ New protein token sequence generated successfully!\")\n",
    "print(f\"\\n--- Generated Token Sequence (first 15 tokens) ---\\n{generated_tokens[:15]}\")\n",
    "\n",
    "\n",
    "generated_coords = kmeans.cluster_centers_[generated_tokens]\n",
    "\n",
    "\n",
    "output_protein_path = \"../results/generated_proteins/novel_protein_1.xyz\"\n",
    "\n",
    "with open(output_protein_path, \"w\") as f:\n",
    "    f.write(f\"{len(generated_coords)}\\n\")\n",
    "    f.write(\"Generated by Proteus AI\\n\") \n",
    "    for coord in generated_coords:\n",
    "       \n",
    "        f.write(f\"C {coord[0]:.3f} {coord[1]:.3f} {coord[2]:.3f}\\n\")\n",
    "\n",
    "print(f\"\\n‚≠êÔ∏è Your new protein has been saved to: {output_protein_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
